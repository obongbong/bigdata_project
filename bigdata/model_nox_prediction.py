import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib
import joblib

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# í•œê¸€ í°íŠ¸ ì„¤ì • (Windowsìš©: 'Malgun Gothic')
matplotlib.rcParams['font.family'] = 'Malgun Gothic'
matplotlib.rcParams['axes.unicode_minus'] = False

# 1. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv("data/ë³‘í•©ëœ_NOX_ë°ì´í„°ì…‹.csv")

# 2. ë²”ì£¼í˜• ì¸ì½”ë”©
df = pd.get_dummies(df, columns=["ì‚¬ì—…ì†Œ", "í˜¸ê¸°"])

# 3. íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ì„¤ì •
X = df[["NOX_ì¼í‰ê· ", "SOX_ì¼í‰ê· ", "ë¨¼ì§€", "ì‚°ì†Œ", "ìœ ëŸ‰", "ì˜¨ë„"] +
       [col for col in df.columns if col.startswith("ì‚¬ì—…ì†Œ_") or col.startswith("í˜¸ê¸°_")]]
y = df["ì§ˆì†Œì‚°í™”ë¬¼(ppm)í‰ê· "]

# 4. í•™ìŠµ/ê²€ì¦ ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5. GridSearchCV ê¸°ë°˜ ìµœì  ëª¨ë¸ íƒìƒ‰
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
}
grid_search = GridSearchCV(RandomForestRegressor(random_state=42), param_grid, cv=3, n_jobs=-1)
grid_search.fit(X_train, y_train)

best_model = grid_search.best_estimator_
print("\nğŸ“Œ Best Params:", grid_search.best_params_)

# 6. ì˜ˆì¸¡ ë° í‰ê°€
y_pred = best_model.predict(X_test)
rmse = mean_squared_error(y_test, y_pred, squared=False)
r2 = r2_score(y_test, y_pred)

print(f"\nğŸ“ˆ RMSE: {rmse:.3f}")
print(f"ğŸ“ˆ RÂ²: {r2:.3f}")

# 7. ë³€ìˆ˜ ì¤‘ìš”ë„ ì‹œê°í™”
importances = best_model.feature_importances_
feature_names = X.columns

plt.figure(figsize=(10, 6))
plt.barh(feature_names, importances)
plt.xlabel("Feature Importance")
plt.title("Random Forest Feature Importance")
plt.tight_layout()
plt.show()

importance_df = pd.DataFrame({
    "feature": feature_names,
    "importance": importances
}).sort_values(by="importance", ascending=False)
print("\nğŸ“Œ NOxì— ì˜í–¥ í° ìƒìœ„ 5ê°œ ì¸ì:")
print(importance_df.head(5))

# 8. ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’ ì‹œê°í™”
plt.figure(figsize=(8, 6))
plt.scatter(y_test, y_pred, alpha=0.7)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel("Actual NOx")
plt.ylabel("Predicted NOx")
plt.title("Actual vs Predicted NOx")
plt.grid(True)
plt.tight_layout()
plt.show()

# 9. ê·œì œ ì´ˆê³¼ ì˜ˆì¸¡ ë¶„ì„
regulation_limit = 50
exceed_pred = y_pred > regulation_limit
exceed_rate = np.mean(exceed_pred)
print(f"\nğŸš¨ ê·œì œ ê¸°ì¤€ {regulation_limit}ppm ì´ˆê³¼ ì˜ˆì¸¡ ë¹„ìœ¨: {exceed_rate*100:.2f}%")

exceed_cases = pd.DataFrame({
    "Actual": y_test,
    "Predicted": y_pred,
    "ì´ˆê³¼ì—¬ë¶€": exceed_pred
})
print("\nâš ï¸ ì˜ˆì¸¡ ê¸°ì¤€ ì´ˆê³¼ ìƒ˜í”Œ:")
print(exceed_cases[exceed_cases["ì´ˆê³¼ì—¬ë¶€"]].head())

# 10. GridSearch ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”
cv_results = pd.DataFrame(grid_search.cv_results_)
result_summary = cv_results[["params", "mean_test_score", "std_test_score", "rank_test_score"]]
result_summary = result_summary.sort_values(by="mean_test_score", ascending=False)
result_summary.to_csv("gridsearch_nox_results.csv", index=False)

print("\nğŸ“Š GridSearch ìƒìœ„ 10ê°œ ì¡°í•©:")
print(result_summary.head(10))

cv_results["param_summary"] = cv_results["params"].astype(str)
plt.figure(figsize=(12, 6))
sns.barplot(x="mean_test_score", y="param_summary", data=cv_results.sort_values(by="mean_test_score", ascending=False))
plt.title("í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°í•©ë³„ êµì°¨ê²€ì¦ í‰ê·  RÂ²")
plt.xlabel("Mean Test RÂ² Score")
plt.ylabel("Parameter Combination")
plt.tight_layout()
plt.show()

# 11. ìµœì  ëª¨ë¸ ë° feature ëª©ë¡ ì €ì¥
os.makedirs("models", exist_ok=True)
joblib.dump(best_model, "models/best_nox_model.pkl")

# ğŸ” í•™ìŠµì— ì‚¬ìš©ëœ feature ì´ë¦„ ì €ì¥
with open("models/feature_names.txt", "w", encoding="utf-8") as f:
    for col in X.columns:
        f.write(col + "\n")

print("\nğŸ’¾ ëª¨ë¸ ë° feature ëª©ë¡ì´ models í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
